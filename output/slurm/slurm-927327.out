cpu-bind=MASK - bp1-gpu005, task 4294967295 4294967295 [0]: mask 0x1010 set
Tue Feb  1 11:47:55 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |
| 38%   64C    P0    76W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:AF:00.0 Off |                  N/A |
| 15%   63C    P0    63W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Tue_Sep_15_19:10:02_PDT_2020
Cuda compilation tools, release 11.1, V11.1.74
Build cuda_11.1.TC455_06.29069683_0
My torch test
Running on host bp1-gpu005.data.bp.acrc.priv
Time is Tue 1 Feb 11:47:58 GMT 2022
Directory is /user/home/ak18001/code/colloidoscope
Slurm job ID is 927327
This jobs runs on the following machines:
bp1-gpu005
16
Current cuda device  0
True
------------num available devices: 2
https://app.neptune.ai/wahabk/Colloidoscope/e/COL-173
/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/neptune/new/internal/utils/git.py:35: UserWarning: GitPython could not be initialized
  warnings.warn("GitPython could not be initialized")
/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/neptune/new/internal/utils/git.py:35: UserWarning: GitPython could not be initialized
  warnings.warn("GitPython could not be initialized")
/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/neptune/new/internal/utils/git.py:35: UserWarning: GitPython could not be initialized
  warnings.warn("GitPython could not be initialized")
/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
training on cuda
Progress:   0%|          | 0/30 [00:00<?, ?it/s]
Training:   0%|          | 0/63 [00:00<?, ?it/s][A
                                                [AProgress:   0%|          | 0/30 [00:14<?, ?it/s]
Traceback (most recent call last):
  File "scripts/bp1.py", line 48, in <module>
    train(config, name, dataset_path=dataset_path, dataset_name=dataset_name, 
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/colloidoscope/trainer.py", line 436, in train
    training_losses, validation_losses, lr_rates = trainer.run_trainer()
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/colloidoscope/trainer.py", line 65, in run_trainer
    self._train()
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/colloidoscope/trainer.py", line 104, in _train
    out = self.model(input_)  # one forward pass
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/colloidoscope/unet.py", line 472, in forward
    x, before_pooling = module(x)
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/colloidoscope/unet.py", line 213, in forward
    y = self.conv1(x)  # convolution 1
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 590, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/user/home/ak18001/.conda/envs/colloids/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 585, in _conv_forward
    return F.conv3d(
RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.76 GiB total capacity; 9.22 GiB already allocated; 389.44 MiB free; 9.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Shutting down background jobs, please wait a moment...
Done!
Waiting for the remaining 59 operations to synchronize with Neptune. Do not kill this process.
All 59 operations synced, thanks for waiting!
